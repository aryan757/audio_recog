{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\speech_recognition\\speech_recognition\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(\"aryan_audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0349e-06, -4.4761e-06, -1.0259e-05,  ...,  9.8359e-04,\n",
       "          2.4207e-03,  3.2772e-03]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(waveform[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate\n",
    "\n",
    "#A sample rate is the number of audio measurements taken per second when converting sound into digital form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "resample() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: resample() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "waveform = librosa.resample(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\speech_recognition\\speech_recognition\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\speech_recognition\\speech_recognition\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_and_preprocess_data' from 'train' (e:\\speech_recognition\\train.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VoiceRecognitionModel\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_and_preprocess_data, train_model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_and_preprocess_data' from 'train' (e:\\speech_recognition\\train.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from model import VoiceRecognitionModel\n",
    "from train import load_and_preprocess_data, train_model\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_voice(file_path):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    # Extract features like MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    return np.mean(mfcc.T, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "\n",
    "class VoiceRecognitionModel:\n",
    "    def __init__(self):\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "    def train(self, train_data, labels):\n",
    "        # Prepare input tensors\n",
    "        inputs = self.processor(train_data, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        labels = torch.tensor(labels).unsqueeze(0)\n",
    "        # Training process (simplified)\n",
    "        self.model.train()\n",
    "        outputs = self.model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "    def predict(self, audio):\n",
    "        inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        logits = self.model(**inputs).logits\n",
    "        predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "        return predicted_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_voice(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "# Example: Process your three voice samples\n",
    "voice1_features = preprocess_voice('nirupam_audio.mp3')\n",
    "voice2_features = preprocess_voice('aryan_audio.mp3')\n",
    "voice3_features = preprocess_voice('ashutosh_audio.mp3')\n",
    "\n",
    "# Prepare the training data\n",
    "train_data = np.array([voice1_features, voice2_features, voice3_features])\n",
    "labels = np.array([0, 1, 2])  # Assign unique labels to each voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 8.4663\n",
      "Epoch [20/100], Loss: 0.1742\n",
      "Epoch [40/100], Loss: 0.0734\n",
      "Epoch [60/100], Loss: 0.0416\n",
      "Epoch [80/100], Loss: 0.0266\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleVoiceRecognitionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleVoiceRecognitionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # Output layer for 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleVoiceRecognitionModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Train for 100 epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_data_tensor)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch [{epoch}/100], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "def predict(model, file_path):\n",
    "    features = preprocess_voice(file_path)\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        output = model(features_tensor)\n",
    "    predicted_label = torch.argmax(output).item()\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "predicted_label = predict(model, 'testing_audio.mp3')\n",
    "print(f'Predicted Label: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 21:19:59.668 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run e:\\speech_recognition\\speech_recognition\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"Voice Recognition System\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload a voice sample\", type=[\"wav\", \"mp3\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    st.audio(uploaded_file, format=\"audio/wav\")\n",
    "    predicted_label = predict(model, uploaded_file)\n",
    "    if predicted_label == 0  :  # Adjust according to your labels\n",
    "        st.success(\"Unlocked!\")\n",
    "    else:\n",
    "        st.error(\"Access Denied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
